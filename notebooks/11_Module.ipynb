{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Module 11: Advanced Topics\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/computer-graphics/blob/main/notebooks/11_Module.ipynb)\n",
    "\n",
    "**Week 16: Image Pyramids, Panorama Stitching, Template Matching, Video Processing**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand image pyramid theory and applications\n",
    "- Implement multi-resolution processing\n",
    "- Create panoramic images using stitching\n",
    "- Apply template matching for object detection\n",
    "- Process real-time video streams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-warning",
   "metadata": {},
   "source": [
    "---\n",
    "## âš ï¸ IMPORTANT: Run All Cells in Order\n",
    "\n",
    "**This notebook must be executed sequentially from top to bottom.**\n",
    "\n",
    "- Click **Runtime â†’ Run all** (or **Cell â†’ Run All**)\n",
    "- Do NOT skip cells or run them out of order\n",
    "- Each cell depends on variables from previous cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-pyramids-theory",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Image Pyramids\n",
    "\n",
    "An **image pyramid** is a multi-scale representation of an image.\n",
    "\n",
    "### Gaussian Pyramid\n",
    "\n",
    "**Level 0**: Original image $G_0$\n",
    "\n",
    "**Level i+1**: Downsample level i\n",
    "$$\n",
    "G_{i+1} = \\text{downsample}(G_i * g)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g$: Gaussian kernel (typically 5Ã—5)\n",
    "- downsample: Remove every other row and column\n",
    "\n",
    "**Result**: Each level is 1/4 the size of previous\n",
    "\n",
    "### Laplacian Pyramid\n",
    "\n",
    "Stores **difference** between levels:\n",
    "$$\n",
    "L_i = G_i - \\text{expand}(G_{i+1})\n",
    "$$\n",
    "\n",
    "Where expand: Upsample and blur\n",
    "\n",
    "**Properties**:\n",
    "- Reconstructable: $G_i = L_i + \\text{expand}(G_{i+1})$\n",
    "- Bandpass filtered\n",
    "- Used for blending\n",
    "\n",
    "### Applications\n",
    "\n",
    "1. **Multi-scale processing**: Detect features at different scales\n",
    "2. **Image blending**: Seamless panoramas\n",
    "3. **Compression**: Store differences instead of full images\n",
    "4. **Template matching**: Find objects at multiple scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pyramids-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image pyramids demonstration",
    "print(\"=\" * 70)",
    "print(\"IMAGE PYRAMIDS\")",
    "print(\"=\" * 70)",
    "",
    "# Load image",
    "import urllib.request",
    "from urllib.request import Request, urlopen",
    "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/480px-Cat03.jpg'",
    "urllib.request.urlretrieve(url, 'test.jpg')",
    "img = cv2.imread('test.jpg')",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)",
    "",
    "# Create Gaussian pyramid",
    "num_levels = 4",
    "gaussian_pyr = [img.copy()]",
    "",
    "print(f\"\\nBuilding Gaussian pyramid with {num_levels} levels...\")",
    "for i in range(num_levels - 1):",
    "    img = cv2.pyrDown(img)",
    "    gaussian_pyr.append(img)",
    "    print(f\"  Level {i+1}: {img.shape[1]}Ã—{img.shape[0]}\")",
    "",
    "# Create Laplacian pyramid",
    "laplacian_pyr = []",
    "print(f\"\\nBuilding Laplacian pyramid...\")",
    "",
    "for i in range(num_levels - 1):",
    "    # Expand next level",
    "    expanded = cv2.pyrUp(gaussian_pyr[i + 1], dstsize=(gaussian_pyr[i].shape[1], gaussian_pyr[i].shape[0]))",
    "    # Subtract to get Laplacian",
    "    laplacian = cv2.subtract(gaussian_pyr[i], expanded)",
    "    laplacian_pyr.append(laplacian)",
    "    print(f\"  Level {i}: {laplacian.shape[1]}Ã—{laplacian.shape[0]}\")",
    "",
    "# Add the smallest level",
    "laplacian_pyr.append(gaussian_pyr[-1])",
    "",
    "# Visualize Gaussian pyramid",
    "fig, axes = plt.subplots(1, num_levels, figsize=(16, 4))",
    "for i, gp in enumerate(gaussian_pyr):",
    "    axes[i].imshow(cv2.cvtColor(gp, cv2.COLOR_BGR2RGB))",
    "    axes[i].set_title(f'Gaussian Level {i}\\n{gp.shape[1]}Ã—{gp.shape[0]}')",
    "    axes[i].axis('off')",
    "plt.suptitle('Gaussian Pyramid', fontsize=14)",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "# Visualize Laplacian pyramid",
    "fig, axes = plt.subplots(1, num_levels, figsize=(16, 4))",
    "for i, lp in enumerate(laplacian_pyr):",
    "    # Normalize for visualization",
    "    lp_vis = cv2.normalize(lp, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)",
    "    axes[i].imshow(cv2.cvtColor(lp_vis, cv2.COLOR_BGR2RGB))",
    "    axes[i].set_title(f'Laplacian Level {i}\\n{lp.shape[1]}Ã—{lp.shape[0]}')",
    "    axes[i].axis('off')",
    "plt.suptitle('Laplacian Pyramid', fontsize=14)",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "# Reconstruct from Laplacian pyramid",
    "print(f\"\\nReconstructing from Laplacian pyramid...\")",
    "reconstructed = laplacian_pyr[-1]",
    "for i in range(num_levels - 2, -1, -1):",
    "    reconstructed = cv2.pyrUp(reconstructed, dstsize=(laplacian_pyr[i].shape[1], laplacian_pyr[i].shape[0]))",
    "    reconstructed = cv2.add(reconstructed, laplacian_pyr[i])",
    "",
    "# Compare original and reconstructed",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))",
    "axes[0].imshow(cv2.cvtColor(gaussian_pyr[0], cv2.COLOR_BGR2RGB))",
    "axes[0].set_title('Original')",
    "axes[0].axis('off')",
    "",
    "axes[1].imshow(cv2.cvtColor(reconstructed, cv2.COLOR_BGR2RGB))",
    "axes[1].set_title('Reconstructed')",
    "axes[1].axis('off')",
    "",
    "# Difference",
    "diff = cv2.absdiff(gaussian_pyr[0], reconstructed)",
    "axes[2].imshow(cv2.cvtColor(diff, cv2.COLOR_BGR2RGB))",
    "axes[2].set_title('Difference (amplified)')",
    "axes[2].axis('off')",
    "",
    "plt.tight_layout()",
    "plt.show()",
    "",
    "print(\"\\nKey Insight: Pyramids enable multi-scale image processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-template-theory",
   "metadata": {},
   "source": [
    "## 2. Template Matching\n",
    "\n",
    "**Template matching** finds occurrences of a template image within a larger image.\n",
    "\n",
    "### Methods\n",
    "\n",
    "**1. Square Difference**:\n",
    "$$\n",
    "R(x, y) = \\sum_{x', y'} [T(x', y') - I(x + x', y + y')]^2\n",
    "$$\n",
    "\n",
    "**2. Normalized Square Difference**:\n",
    "$$\n",
    "R(x, y) = \\frac{\\sum_{x', y'} [T(x', y') - I(x + x', y + y')]^2}{\\sqrt{\\sum_{x', y'} T(x', y')^2 \\cdot \\sum_{x', y'} I(x + x', y + y')^2}}\n",
    "$$\n",
    "\n",
    "**3. Cross-Correlation**:\n",
    "$$\n",
    "R(x, y) = \\sum_{x', y'} T(x', y') \\cdot I(x + x', y + y')\n",
    "$$\n",
    "\n",
    "**4. Normalized Cross-Correlation** (best):\n",
    "$$\n",
    "R(x, y) = \\frac{\\sum_{x', y'} T(x', y') \\cdot I(x + x', y + y')}{\\sqrt{\\sum_{x', y'} T(x', y')^2 \\cdot \\sum_{x', y'} I(x + x', y + y')^2}}\n",
    "$$\n",
    "\n",
    "### OpenCV Methods\n",
    "\n",
    "- `TM_SQDIFF`: Square difference (minimum is best)\n",
    "- `TM_SQDIFF_NORMED`: Normalized square difference\n",
    "- `TM_CCORR`: Cross-correlation (maximum is best)\n",
    "- `TM_CCORR_NORMED`: Normalized cross-correlation\n",
    "- `TM_CCOEFF`: Correlation coefficient\n",
    "- `TM_CCOEFF_NORMED`: Normalized correlation coefficient\n",
    "\n",
    "**Recommendation**: Use `TM_CCOEFF_NORMED` (robust to brightness changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-template-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template matching demonstration\n",
    "print(\"=\" * 70)\n",
    "print(\"TEMPLATE MATCHING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load image and template (use a crop as template)\n",
    "img_full = cv2.imread('test.jpg')\n",
    "h, w = img_full.shape[:2]\n",
    "\n",
    "# Create template (crop from image)\n",
    "template = img_full[h//4:h//4+100, w//4:w//4+100]\n",
    "th, tw = template.shape[:2]\n",
    "\n",
    "print(f\"\\nImage size: {w}Ã—{h}\")\n",
    "print(f\"Template size: {tw}Ã—{th}\")\n",
    "\n",
    "# All template matching methods\n",
    "methods = [\n",
    "    ('TM_CCOEFF', cv2.TM_CCOEFF),\n",
    "    ('TM_CCOEFF_NORMED', cv2.TM_CCOEFF_NORMED),\n",
    "    ('TM_CCORR', cv2.TM_CCORR),\n",
    "    ('TM_CCORR_NORMED', cv2.TM_CCORR_NORMED),\n",
    "    ('TM_SQDIFF', cv2.TM_SQDIFF),\n",
    "    ('TM_SQDIFF_NORMED', cv2.TM_SQDIFF_NORMED)\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Show original and template\n",
    "axes[0].imshow(cv2.cvtColor(img_full, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(template, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title('Template')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].axis('off')\n",
    "\n",
    "print(f\"\\nTesting {len(methods)} matching methods:\\n\")\n",
    "\n",
    "for idx, (name, method) in enumerate(methods):\n",
    "    # Apply template matching\n",
    "    result = cv2.matchTemplate(img_full, template, method)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    \n",
    "    # For SQDIFF methods, minimum is best match\n",
    "    if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
    "        top_left = min_loc\n",
    "        match_val = min_val\n",
    "    else:\n",
    "        top_left = max_loc\n",
    "        match_val = max_val\n",
    "    \n",
    "    bottom_right = (top_left[0] + tw, top_left[1] + th)\n",
    "    \n",
    "    print(f\"{name:20s}: match={match_val:.4f}, location={top_left}\")\n",
    "    \n",
    "    # Draw result\n",
    "    img_display = img_full.copy()\n",
    "    cv2.rectangle(img_display, top_left, bottom_right, (0, 255, 0), 3)\n",
    "    \n",
    "    axes[idx + 3].imshow(cv2.cvtColor(img_display, cv2.COLOR_BGR2RGB))\n",
    "    axes[idx + 3].set_title(f'{name}\\nScore: {match_val:.3f}')\n",
    "    axes[idx + 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Template matching finds objects at fixed scale!\")\n",
    "print(\"For scale-invariant detection, use feature matching instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-panorama-theory",
   "metadata": {},
   "source": [
    "## 3. Image Stitching (Panorama)\n",
    "\n",
    "**Image stitching** combines multiple overlapping images into a panorama.\n",
    "\n",
    "### Algorithm Steps\n",
    "\n",
    "1. **Detect features** in all images (SIFT, ORB)\n",
    "2. **Match features** between image pairs\n",
    "3. **Estimate homography** $H$ between images:\n",
    "   $$\n",
    "   \\begin{bmatrix} x_2 \\\\ y_2 \\\\ 1 \\end{bmatrix} = H \\begin{bmatrix} x_1 \\\\ y_1 \\\\ 1 \\end{bmatrix}\n",
    "   $$\n",
    "4. **RANSAC** to find robust homography\n",
    "5. **Warp images** to common coordinate frame\n",
    "6. **Blend** overlapping regions\n",
    "\n",
    "### Homography Estimation\n",
    "\n",
    "Need at least **4 point correspondences**.\n",
    "\n",
    "Solve system:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1 & y_1 & 1 & 0 & 0 & 0 & -x_2 x_1 & -x_2 y_1 \\\\\n",
    "0 & 0 & 0 & x_1 & y_1 & 1 & -y_2 x_1 & -y_2 y_1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\\\ h_4 \\\\ h_5 \\\\ h_6 \\\\ h_7 \\\\ h_8 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} x_2 \\\\ y_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### RANSAC (Random Sample Consensus)\n",
    "\n",
    "Robust estimation in presence of outliers:\n",
    "\n",
    "1. **Sample** 4 random matches\n",
    "2. **Compute** homography from sample\n",
    "3. **Count** inliers (matches consistent with H)\n",
    "4. **Repeat** N times\n",
    "5. **Select** H with most inliers\n",
    "6. **Recompute** H using all inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-panorama-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panorama stitching demonstration\n",
    "print(\"=\" * 70)\n",
    "print(\"PANORAMA STITCHING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# For demonstration, create two overlapping images from one\n",
    "img_orig = cv2.imread('test.jpg')\n",
    "h, w = img_orig.shape[:2]\n",
    "\n",
    "# Create overlapping images (simulating panorama pieces)\n",
    "img1 = img_orig[:, :int(w*0.6)]\n",
    "img2 = img_orig[:, int(w*0.4):]\n",
    "\n",
    "print(f\"\\nImage 1: {img1.shape[1]}Ã—{img1.shape[0]}\")\n",
    "print(f\"Image 2: {img2.shape[1]}Ã—{img2.shape[0]}\")\n",
    "print(f\"Overlap region: ~{int(w*0.2)} pixels\")\n",
    "\n",
    "# Detect ORB features\n",
    "orb = cv2.ORB_create(nfeatures=2000)\n",
    "kp1, des1 = orb.detectAndCompute(img1, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "print(f\"\\nFeatures detected:\")\n",
    "print(f\"  Image 1: {len(kp1)} keypoints\")\n",
    "print(f\"  Image 2: {len(kp2)} keypoints\")\n",
    "\n",
    "# Match features\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "print(f\"  Matches found: {len(matches)}\")\n",
    "\n",
    "# Extract matched points\n",
    "if len(matches) > 4:\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    \n",
    "    # Find homography using RANSAC\n",
    "    H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    inliers = mask.ravel().tolist().count(1)\n",
    "    print(f\"\\nRANSAC results:\")\n",
    "    print(f\"  Inliers: {inliers} / {len(matches)}\")\n",
    "    print(f\"  Homography matrix H:\")\n",
    "    print(H)\n",
    "    \n",
    "    # Warp img2 to img1's coordinate frame\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    \n",
    "    # Calculate output size\n",
    "    pts1 = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)\n",
    "    pts2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "    pts2_transformed = cv2.perspectiveTransform(pts2, H)\n",
    "    \n",
    "    pts = np.concatenate((pts1, pts2_transformed), axis=0)\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    \n",
    "    # Translation to keep everything positive\n",
    "    t = [-xmin, -ymin]\n",
    "    Ht = np.array([[1, 0, t[0]], [0, 1, t[1]], [0, 0, 1]])\n",
    "    \n",
    "    # Warp and stitch\n",
    "    result = cv2.warpPerspective(img2, Ht @ H, (xmax - xmin, ymax - ymin))\n",
    "    result[t[1]:h1 + t[1], t[0]:w1 + t[0]] = img1\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 0].set_title('Image 1 (Left)')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "    axes[0, 1].set_title('Image 2 (Right)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None,\n",
    "                                   flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    axes[1, 0].imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 0].set_title(f'Feature Matches (top 50)\\n{inliers} inliers total')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(cv2.cvtColor(result, cv2.COLOR_BGR2RGB))\n",
    "    axes[1, 1].set_title('Stitched Panorama')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPanorama size: {result.shape[1]}Ã—{result.shape[0]}\")\n",
    "else:\n",
    "    print(\"\\nNot enough matches for homography estimation\")\n",
    "\n",
    "print(\"\\nKey Insight: Panoramas combine multiple views using feature matching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-video-theory",
   "metadata": {},
   "source": [
    "## 4. Video Processing\n",
    "\n",
    "**Video** is a sequence of frames (images).\n",
    "\n",
    "### Video Capture\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(0)  # 0 = default camera\n",
    "cap = cv2.VideoCapture('video.mp4')  # From file\n",
    "```\n",
    "\n",
    "### Reading Frames\n",
    "\n",
    "```python\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Process frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "### Video Properties\n",
    "\n",
    "- `CAP_PROP_FRAME_WIDTH`: Frame width\n",
    "- `CAP_PROP_FRAME_HEIGHT`: Frame height\n",
    "- `CAP_PROP_FPS`: Frames per second\n",
    "- `CAP_PROP_FRAME_COUNT`: Total frames\n",
    "\n",
    "### Video Writing\n",
    "\n",
    "```python\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "\n",
    "for frame in frames:\n",
    "    out.write(frame)\n",
    "\n",
    "out.release()\n",
    "```\n",
    "\n",
    "### Real-time Processing Tips\n",
    "\n",
    "1. **Reduce resolution**: Process smaller frames\n",
    "2. **Skip frames**: Process every Nth frame\n",
    "3. **Multi-threading**: Separate capture and processing\n",
    "4. **GPU acceleration**: Use CUDA if available\n",
    "5. **Optimize algorithms**: Use fast methods (ORB > SIFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-video-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video processing example (conceptual code)\n",
    "print(\"=\" * 70)\n",
    "print(\"VIDEO PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "Video processing typically requires:\n",
    "1. Access to camera or video file\n",
    "2. Real-time display window\n",
    "\n",
    "In Colab/Jupyter, video capture is limited. \n",
    "Here's the standard workflow:\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "\n",
    "# Open camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open camera\")\n",
    "    exit()\n",
    "\n",
    "# Get video properties\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(f\"Video: {width}Ã—{height} @ {fps} FPS\")\n",
    "\n",
    "while True:\n",
    "    # Capture frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Can't receive frame\")\n",
    "        break\n",
    "    \n",
    "    # Process frame (example: convert to grayscale)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply edge detection\n",
    "    edges = cv2.Canny(gray, 100, 200)\n",
    "    \n",
    "    # Display result\n",
    "    cv2.imshow('Original', frame)\n",
    "    cv2.imshow('Edges', edges)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "```\n",
    "\n",
    "Common video processing applications:\n",
    "- Object detection and tracking\n",
    "- Motion detection\n",
    "- Face detection/recognition\n",
    "- Optical flow\n",
    "- Background subtraction\n",
    "- Activity recognition\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nKey Insight: Video = sequence of images processed in real-time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Image Pyramids**: Multi-scale representation\n",
    "2. **Template Matching**: Find patterns at fixed scale\n",
    "3. **Panorama Stitching**: Combine images using homography\n",
    "4. **Video Processing**: Real-time frame-by-frame analysis\n",
    "\n",
    "### Pyramid Applications\n",
    "\n",
    "| Type | Use Case | Storage |\n",
    "|------|----------|----------|\n",
    "| Gaussian | Multi-scale detection | 1.33Ã— original |\n",
    "| Laplacian | Image blending | 1.33Ã— original |\n",
    "| Steerable | Orientation analysis | Varies |\n",
    "\n",
    "### Mathematical Formulas\n",
    "\n",
    "**Gaussian pyramid**:\n",
    "$$\n",
    "G_{i+1} = \\text{downsample}(G_i * g)\n",
    "$$\n",
    "\n",
    "**Laplacian pyramid**:\n",
    "$$\n",
    "L_i = G_i - \\text{expand}(G_{i+1})\n",
    "$$\n",
    "\n",
    "**Homography**:\n",
    "$$\n",
    "\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Template matching (normalized correlation)**:\n",
    "$$\n",
    "R(x,y) = \\frac{\\sum_{x',y'} T(x',y') \\cdot I(x+x', y+y')}{\\sqrt{\\sum T^2 \\cdot \\sum I^2}}\n",
    "$$\n",
    "\n",
    "### OpenCV Functions\n",
    "\n",
    "| Operation | Function | Key Parameters |\n",
    "|-----------|----------|----------------|\n",
    "| Pyramid down | `cv2.pyrDown(img)` | Reduces size by 2 |\n",
    "| Pyramid up | `cv2.pyrUp(img)` | Increases size by 2 |\n",
    "| Template match | `cv2.matchTemplate(img, tmpl, method)` | TM_CCOEFF_NORMED |\n",
    "| Find homography | `cv2.findHomography(src, dst, method)` | RANSAC |\n",
    "| Warp perspective | `cv2.warpPerspective(img, H, size)` | Uses 3Ã—3 matrix |\n",
    "| Video capture | `cv2.VideoCapture(source)` | 0 for camera |\n",
    "| Video write | `cv2.VideoWriter(file, fourcc, fps, size)` | XVID, MP4V |\n",
    "\n",
    "## Course Complete!\n",
    "\n",
    "Congratulations! You've completed all 11 modules covering:\n",
    "\n",
    "âœ… Image fundamentals and color spaces  \n",
    "âœ… Filtering and enhancement  \n",
    "âœ… Geometric transformations  \n",
    "âœ… Edge detection  \n",
    "âœ… Feature detection and matching  \n",
    "âœ… Contours and shape analysis  \n",
    "âœ… Histograms and equalization  \n",
    "âœ… Image segmentation  \n",
    "âœ… Advanced topics (pyramids, panoramas, video)  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Practice**: Complete the capstone projects\n",
    "2. **Explore**: Try OpenCV's advanced modules (DNN, tracking)\n",
    "3. **Build**: Create your own computer vision applications\n",
    "4. **Learn more**: Deep learning, 3D vision, SLAM\n",
    "\n",
    "Happy coding! ðŸš€"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}