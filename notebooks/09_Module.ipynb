{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Module 9: Image Segmentation\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/computer-graphics/blob/main/notebooks/09_Module.ipynb)\n",
    "\n",
    "**Week 14: Thresholding, Watershed, GrabCut, K-means Clustering**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand image segmentation theory and methods\n",
    "- Apply various thresholding techniques (Otsu, adaptive)\n",
    "- Implement Watershed algorithm for segmentation\n",
    "- Use GrabCut for foreground extraction\n",
    "- Apply K-means clustering for color-based segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-warning",
   "metadata": {},
   "source": [
    "---\n",
    "## ⚠️ IMPORTANT: Run All Cells in Order\n",
    "\n",
    "**This notebook must be executed sequentially from top to bottom.**\n",
    "\n",
    "- Click **Runtime → Run all** (or **Cell → Run All**)\n",
    "- Do NOT skip cells or run them out of order\n",
    "- Each cell depends on variables from previous cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-theory-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Mathematical Foundations: Segmentation Theory\n",
    "\n",
    "### What is Image Segmentation?\n",
    "\n",
    "**Image segmentation** partitions an image into meaningful regions:\n",
    "$$\n",
    "I = \\bigcup_{i=1}^{n} R_i, \\quad R_i \\cap R_j = \\emptyset \\text{ for } i \\neq j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $I$: Image\n",
    "- $R_i$: Regions (segments)\n",
    "- Each pixel belongs to exactly one region\n",
    "\n",
    "### Why Segmentation?\n",
    "\n",
    "1. **Object detection**: Separate objects from background\n",
    "2. **Image understanding**: Identify regions of interest\n",
    "3. **Measurement**: Quantify object properties\n",
    "4. **Recognition**: Classify segmented regions\n",
    "5. **Medical imaging**: Delineate anatomical structures\n",
    "\n",
    "### Segmentation Categories\n",
    "\n",
    "**1. Thresholding**: Based on intensity values\n",
    "**2. Region-based**: Grow or split regions\n",
    "**3. Edge-based**: Use boundary information\n",
    "**4. Clustering**: Group similar pixels\n",
    "**5. Graph-based**: Model image as graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-images",
   "metadata": {},
   "outputs": [],
   "source": "# Load test images\nimport urllib.request\nfrom urllib.request import Request, urlopen\n\n# Simple image for thresholding - use a different reliable image\n# Using a building/architecture image that works well for segmentation\nurl1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/300px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg'\nreq1 = Request(url1, headers={'User-Agent': 'Mozilla/5.0'})\nwith urlopen(req1) as response:\n    image_data1 = response.read()\nwith open('coins.jpg', 'wb') as f:\n    f.write(image_data1)\n\nimg_coins = cv2.imread('coins.jpg')\nimg_coins_gray = cv2.cvtColor(img_coins, cv2.COLOR_BGR2GRAY)\n\n# Complex image for advanced methods\nurl2 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg'\nreq2 = Request(url2, headers={'User-Agent': 'Mozilla/5.0'})\nwith urlopen(req2) as response:\n    image_data2 = response.read()\nwith open('cat.jpg', 'wb') as f:\n    f.write(image_data2)\n\nimg_cat = cv2.imread('cat.jpg')\n\nprint(f\"Coins image: {img_coins.shape}\")\nprint(f\"Cat image: {img_cat.shape}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].imshow(cv2.cvtColor(img_coins, cv2.COLOR_BGR2RGB))\naxes[0].set_title('Test Image 1')\naxes[0].axis('off')\n\naxes[1].imshow(cv2.cvtColor(img_cat, cv2.COLOR_BGR2RGB))\naxes[1].set_title('Test Image 2 (Cat)')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-thresholding-theory",
   "metadata": {},
   "source": [
    "### 1. Thresholding\n",
    "\n",
    "**Thresholding** creates binary images by comparing pixels to threshold(s).\n",
    "\n",
    "#### Simple Thresholding\n",
    "\n",
    "$$\n",
    "g(x, y) = \\begin{cases}\n",
    "255 & \\text{if } f(x, y) > T \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**Problem**: Choosing optimal threshold $T$?\n",
    "\n",
    "#### Otsu's Method\n",
    "\n",
    "**Idea**: Find threshold that minimizes **intra-class variance** (or maximizes inter-class variance).\n",
    "\n",
    "**Algorithm**:\n",
    "1. Calculate histogram\n",
    "2. For each possible threshold $t$:\n",
    "   - Compute weights: $w_0(t), w_1(t)$\n",
    "   - Compute means: $\\mu_0(t), \\mu_1(t)$\n",
    "   - Compute inter-class variance:\n",
    "     $$\n",
    "     \\sigma_b^2(t) = w_0(t) w_1(t) [\\mu_1(t) - \\mu_0(t)]^2\n",
    "     $$\n",
    "3. Select $t^* = \\arg\\max_t \\sigma_b^2(t)$\n",
    "\n",
    "**Properties**:\n",
    "- Automatic (no manual threshold)\n",
    "- Assumes bimodal histogram\n",
    "- Optimal for separating two classes\n",
    "\n",
    "#### Adaptive Thresholding\n",
    "\n",
    "**Problem**: Global threshold fails with varying illumination.\n",
    "\n",
    "**Solution**: Compute local threshold for each pixel:\n",
    "$$\n",
    "T(x, y) = \\text{mean}(N(x, y)) - C\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N(x, y)$: Neighborhood around $(x, y)$\n",
    "- $C$: Constant (user-defined)\n",
    "\n",
    "**Methods**:\n",
    "- **Mean**: $T = \\text{mean}(N) - C$\n",
    "- **Gaussian**: $T = \\text{weighted_mean}(N) - C$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-thresholding-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding methods comparison\n",
    "print(\"=\" * 70)\n",
    "print(\"THRESHOLDING METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Simple threshold\n",
    "_, thresh_simple = cv2.threshold(img_coins_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Otsu's threshold\n",
    "thresh_otsu_val, thresh_otsu = cv2.threshold(img_coins_gray, 0, 255, \n",
    "                                              cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Adaptive threshold (mean)\n",
    "thresh_adaptive_mean = cv2.adaptiveThreshold(img_coins_gray, 255,\n",
    "                                             cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                             cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "# Adaptive threshold (Gaussian)\n",
    "thresh_adaptive_gauss = cv2.adaptiveThreshold(img_coins_gray, 255,\n",
    "                                              cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                              cv2.THRESH_BINARY, 11, 2)\n",
    "\n",
    "print(f\"\\nOtsu's optimal threshold: {thresh_otsu_val:.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].imshow(img_coins_gray, cmap='gray')\n",
    "axes[0, 0].set_title('Original Grayscale')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Histogram with threshold\n",
    "axes[0, 1].hist(img_coins_gray.ravel(), bins=256, range=[0, 256], color='black', alpha=0.7)\n",
    "axes[0, 1].axvline(127, color='blue', linestyle='--', linewidth=2, label='Simple (127)')\n",
    "axes[0, 1].axvline(thresh_otsu_val, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Otsu ({thresh_otsu_val:.0f})')\n",
    "axes[0, 1].set_title('Histogram with Thresholds')\n",
    "axes[0, 1].set_xlabel('Intensity')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 2].imshow(thresh_simple, cmap='gray')\n",
    "axes[0, 2].set_title('Simple Threshold (T=127)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(thresh_otsu, cmap='gray')\n",
    "axes[1, 0].set_title(f\"Otsu's Method (T={thresh_otsu_val:.0f})\")\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(thresh_adaptive_mean, cmap='gray')\n",
    "axes[1, 1].set_title('Adaptive (Mean)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(thresh_adaptive_gauss, cmap='gray')\n",
    "axes[1, 2].set_title('Adaptive (Gaussian)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Otsu automatically finds optimal threshold!\")\n",
    "print(\"Adaptive thresholding handles varying illumination!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-watershed-theory",
   "metadata": {},
   "source": [
    "### 2. Watershed Algorithm\n",
    "\n",
    "The **Watershed algorithm** treats the image as a topographic surface.\n",
    "\n",
    "#### Topographic Interpretation\n",
    "\n",
    "- **Intensity** = altitude\n",
    "- **Local minima** = valleys (catchment basins)\n",
    "- **Watershed lines** = boundaries between basins\n",
    "\n",
    "#### Algorithm (Flooding Simulation)\n",
    "\n",
    "1. **Identify markers** (seeds) for each region\n",
    "2. **Flood** from each marker:\n",
    "   - Water rises from local minima\n",
    "   - When waters from different basins meet → watershed line\n",
    "3. **Label** pixels by their basin\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "**Gradient magnitude** as topographic surface:\n",
    "$$\n",
    "\\nabla f = \\sqrt{(\\frac{\\partial f}{\\partial x})^2 + (\\frac{\\partial f}{\\partial y})^2}\n",
    "$$\n",
    "\n",
    "**Markers**: Binary images indicating object/background seeds\n",
    "\n",
    "#### Marker Selection\n",
    "\n",
    "**Problem**: Over-segmentation (too many basins)\n",
    "\n",
    "**Solution**: Provide markers\n",
    "1. **Foreground markers**: Sure foreground (via morphology)\n",
    "2. **Background markers**: Sure background\n",
    "3. **Unknown region**: Let watershed decide\n",
    "\n",
    "**Common approach**:\n",
    "- Distance transform → local maxima = foreground\n",
    "- Dilation of foreground = background\n",
    "- Between them = unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-watershed-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed segmentation\n",
    "print(\"=\" * 70)\n",
    "print(\"WATERSHED SEGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Preprocess\n",
    "gray = cv2.cvtColor(img_coins, cv2.COLOR_BGR2GRAY)\n",
    "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "# Noise removal\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "# Sure background area\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "# Sure foreground area (distance transform)\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "\n",
    "# Unknown region\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "# Marker labeling\n",
    "_, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = markers + 1  # Add 1 so background is not 0 but 1\n",
    "markers[unknown == 255] = 0  # Mark unknown region as 0\n",
    "\n",
    "# Apply watershed\n",
    "markers_watershed = cv2.watershed(img_coins, markers)\n",
    "\n",
    "# Mark boundaries in red\n",
    "img_result = img_coins.copy()\n",
    "img_result[markers_watershed == -1] = [0, 0, 255]\n",
    "\n",
    "print(f\"\\nNumber of regions found: {markers_watershed.max()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "axes[0, 0].imshow(cv2.cvtColor(img_coins, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('1. Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(thresh, cmap='gray')\n",
    "axes[0, 1].set_title('2. Otsu Threshold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(opening, cmap='gray')\n",
    "axes[0, 2].set_title('3. Morphology Opening')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[0, 3].imshow(dist_transform, cmap='jet')\n",
    "axes[0, 3].set_title('4. Distance Transform')\n",
    "axes[0, 3].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(sure_fg, cmap='gray')\n",
    "axes[1, 0].set_title('5. Sure Foreground')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(sure_bg, cmap='gray')\n",
    "axes[1, 1].set_title('6. Sure Background')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(markers, cmap='nipy_spectral')\n",
    "axes[1, 2].set_title('7. Markers')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[1, 3].imshow(cv2.cvtColor(img_result, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 3].set_title('8. Watershed Result\\n(Red = boundaries)')\n",
    "axes[1, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Watershed segments touching objects using markers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-grabcut-theory",
   "metadata": {},
   "source": [
    "### 3. GrabCut Algorithm\n",
    "\n",
    "**GrabCut** is an interactive foreground extraction algorithm.\n",
    "\n",
    "#### Theory\n",
    "\n",
    "Based on **Graph Cuts** and **Gaussian Mixture Models (GMM)**.\n",
    "\n",
    "**Energy minimization**:\n",
    "$$\n",
    "E(\\alpha, k, \\theta, z) = U(\\alpha, k, \\theta, z) + V(\\alpha, z)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha$: Segmentation (foreground/background)\n",
    "- $k$: GMM components\n",
    "- $\\theta$: GMM parameters\n",
    "- $z$: Image\n",
    "- $U$: Data term (color models)\n",
    "- $V$: Smoothness term (neighboring pixels)\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1. **User input**: Rectangle around object\n",
    "2. **Initialization**: \n",
    "   - Everything outside rectangle = background\n",
    "   - Everything inside = unknown\n",
    "3. **Iterate**:\n",
    "   - Build GMM for foreground and background\n",
    "   - Estimate segmentation (graph cut)\n",
    "   - Learn GMM parameters\n",
    "   - Repeat until convergence\n",
    "\n",
    "#### Input Modes\n",
    "\n",
    "- **GC_INIT_WITH_RECT**: Rectangle initialization\n",
    "- **GC_INIT_WITH_MASK**: User provides mask\n",
    "- **GC_EVAL**: Refine existing result\n",
    "\n",
    "#### Mask Values\n",
    "\n",
    "- **GC_BGD (0)**: Definite background\n",
    "- **GC_FGD (1)**: Definite foreground  \n",
    "- **GC_PR_BGD (2)**: Probably background\n",
    "- **GC_PR_FGD (3)**: Probably foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-grabcut-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrabCut segmentation\n",
    "print(\"=\" * 70)\n",
    "print(\"GRABCUT FOREGROUND EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize mask and models\n",
    "mask = np.zeros(img_cat.shape[:2], np.uint8)\n",
    "bgd_model = np.zeros((1, 65), np.float64)\n",
    "fgd_model = np.zeros((1, 65), np.float64)\n",
    "\n",
    "# Define rectangle around object (adjust for your image)\n",
    "h, w = img_cat.shape[:2]\n",
    "rect = (10, 10, w - 20, h - 20)\n",
    "\n",
    "print(f\"\\nImage size: {w} × {h}\")\n",
    "print(f\"Rectangle: {rect}\")\n",
    "print(f\"Running GrabCut (5 iterations)...\")\n",
    "\n",
    "# Apply GrabCut\n",
    "cv2.grabCut(img_cat, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# Create binary mask\n",
    "# 0 = GC_BGD, 1 = GC_FGD, 2 = GC_PR_BGD, 3 = GC_PR_FGD\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "\n",
    "# Apply mask\n",
    "img_foreground = img_cat * mask2[:, :, np.newaxis]\n",
    "\n",
    "# Alternative: keep probable foreground too\n",
    "mask_prob = np.where((mask == 1) | (mask == 3), 1, 0).astype('uint8')\n",
    "img_foreground_prob = img_cat * mask_prob[:, :, np.newaxis]\n",
    "\n",
    "print(f\"Done!\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Show rectangle on original\n",
    "img_with_rect = img_cat.copy()\n",
    "cv2.rectangle(img_with_rect, (rect[0], rect[1]), \n",
    "                  (rect[0] + rect[2], rect[1] + rect[3]), (0, 255, 0), 3)\n",
    "\n",
    "axes[0, 0].imshow(cv2.cvtColor(img_with_rect, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('1. Original with Rectangle\\n(User input)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show mask values\n",
    "mask_vis = mask.copy()\n",
    "axes[0, 1].imshow(mask_vis, cmap='viridis')\n",
    "axes[0, 1].set_title('2. GrabCut Mask\\n(0=bg, 1=fg, 2=prob_bg, 3=prob_fg)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(mask2, cmap='gray')\n",
    "axes[0, 2].set_title('3. Binary Mask\\n(Definite FG only)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(cv2.cvtColor(img_cat, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title('Original Image')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(cv2.cvtColor(img_foreground, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 1].set_title('4. Extracted Foreground\\n(Definite only)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(cv2.cvtColor(img_foreground_prob, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 2].set_title('5. Extracted Foreground\\n(Including probable)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: GrabCut extracts foreground with minimal user input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-kmeans-theory",
   "metadata": {},
   "source": [
    "### 4. K-means Clustering\n",
    "\n",
    "**K-means** groups pixels into K clusters based on color similarity.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "**Goal**: Minimize within-cluster variance:\n",
    "$$\n",
    "\\arg\\min_S \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\|x - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $S_i$: Cluster $i$\n",
    "- $\\mu_i$: Centroid of cluster $i$\n",
    "- $x$: Data point (pixel)\n",
    "\n",
    "**Steps**:\n",
    "1. **Initialize**: Randomly select K centroids\n",
    "2. **Assignment**: Assign each pixel to nearest centroid\n",
    "   $$\n",
    "   S_i = \\{x : \\|x - \\mu_i\\| \\leq \\|x - \\mu_j\\| \\text{ for all } j\\}\n",
    "   $$\n",
    "3. **Update**: Recompute centroids\n",
    "   $$\n",
    "   \\mu_i = \\frac{1}{|S_i|} \\sum_{x \\in S_i} x\n",
    "   $$\n",
    "4. **Repeat** until convergence\n",
    "\n",
    "#### For Image Segmentation\n",
    "\n",
    "**Feature vector**: RGB values\n",
    "$$\n",
    "x = [R, G, B]^T\n",
    "$$\n",
    "\n",
    "Or include spatial coordinates:\n",
    "$$\n",
    "x = [R, G, B, x, y]^T\n",
    "$$\n",
    "\n",
    "#### OpenCV K-means Parameters\n",
    "\n",
    "- **K**: Number of clusters\n",
    "- **criteria**: Termination criteria (iterations, epsilon)\n",
    "- **attempts**: Number of times algorithm runs with different initial labels\n",
    "- **flags**: Initialization method\n",
    "  - `KMEANS_RANDOM_CENTERS`: Random\n",
    "  - `KMEANS_PP_CENTERS`: K-means++ (better initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-kmeans-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means color clustering\n",
    "print(\"=\" * 70)\n",
    "print(\"K-MEANS COLOR CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def kmeans_segmentation(img, K):\n",
    "    \"\"\"Segment image using K-means clustering\"\"\"\n",
    "    # Reshape image to 2D array of pixels\n",
    "    pixels = img.reshape((-1, 3))\n",
    "    pixels = np.float32(pixels)\n",
    "    \n",
    "    # Define criteria\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
    "    \n",
    "    # Apply K-means\n",
    "    _, labels, centers = cv2.kmeans(pixels, K, None, criteria, 10, \n",
    "                                     cv2.KMEANS_PP_CENTERS)\n",
    "    \n",
    "    # Convert back to uint8\n",
    "    centers = np.uint8(centers)\n",
    "    \n",
    "    # Map pixels to their cluster centers\n",
    "    segmented = centers[labels.flatten()]\n",
    "    segmented = segmented.reshape(img.shape)\n",
    "    \n",
    "    return segmented, labels.reshape(img.shape[:2]), centers\n",
    "\n",
    "# Test with different K values\n",
    "K_values = [2, 4, 8, 16]\n",
    "results = []\n",
    "\n",
    "print(f\"\\nSegmenting with different K values...\\n\")\n",
    "\n",
    "for K in K_values:\n",
    "    print(f\"K={K}...\", end=\" \")\n",
    "    segmented, labels, centers = kmeans_segmentation(img_cat, K)\n",
    "    results.append((segmented, labels, centers))\n",
    "    print(f\"Done! Colors: {centers.tolist()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "# Original\n",
    "axes[0, 0].imshow(cv2.cvtColor(img_cat, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Original Image')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# K-means results\n",
    "positions = [(0, 1), (0, 2), (1, 0), (1, 1)]\n",
    "for (K, (segmented, labels, centers)), (row, col) in zip(\n",
    "    zip(K_values, results), positions):\n",
    "    \n",
    "    axes[row, col].imshow(cv2.cvtColor(segmented, cv2.COLOR_BGR2RGB))\n",
    "    axes[row, col].set_title(f'K={K} clusters')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "# Show cluster labels for K=4\n",
    "_, labels_k4, _ = results[1]  # K=4\n",
    "axes[1, 2].imshow(labels_k4, cmap='nipy_spectral')\n",
    "axes[1, 2].set_title('Cluster Labels (K=4)')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "# Show dominant colors for each K\n",
    "for idx, (K, (_, _, centers)) in enumerate(zip(K_values, results)):\n",
    "    color_bar = np.zeros((50, 50 * K, 3), dtype=np.uint8)\n",
    "    for i, color in enumerate(centers):\n",
    "        color_bar[:, i*50:(i+1)*50] = color\n",
    "    \n",
    "    row = 2\n",
    "    col = idx if idx < 2 else idx - 2\n",
    "    if idx >= 2:\n",
    "        row = 2\n",
    "        col = idx - 2\n",
    "    \n",
    "    axes[2, col].imshow(cv2.cvtColor(color_bar, cv2.COLOR_BGR2RGB))\n",
    "    axes[2, col].set_title(f'K={K} Cluster Centers')\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "# Hide unused subplots\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"K-means Clustering Analysis:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"  K=2:  Very coarse segmentation (2 colors)\")\n",
    "print(\"  K=4:  Good balance (4 major regions)\")\n",
    "print(\"  K=8:  More detailed (8 color regions)\")\n",
    "print(\"  K=16: Fine segmentation (16 colors)\")\n",
    "print(\"\\nKey Insight: K-means segments based on color similarity!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Thresholding**: Binary segmentation (Otsu, adaptive)\n",
    "2. **Watershed**: Region-based, handles touching objects\n",
    "3. **GrabCut**: Interactive foreground extraction\n",
    "4. **K-means**: Color-based clustering\n",
    "\n",
    "### Method Comparison\n",
    "\n",
    "| Method | Input | Automatic | Handles Touching Objects | Best For |\n",
    "|--------|-------|-----------|-------------------------|----------|\n",
    "| **Otsu** | Grayscale | Yes | No | Simple, bimodal images |\n",
    "| **Adaptive** | Grayscale | Yes | No | Varying illumination |\n",
    "| **Watershed** | Gradient + Markers | Semi | **Yes** | Touching/overlapping objects |\n",
    "| **GrabCut** | Color + Rectangle | Semi | Yes | Foreground extraction |\n",
    "| **K-means** | Color | Yes (K needed) | No | Color-based regions |\n",
    "\n",
    "### Mathematical Formulas Reference\n",
    "\n",
    "**Thresholding**:\n",
    "$$\n",
    "g(x,y) = \\begin{cases} 255 & f(x,y) > T \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "**Otsu inter-class variance**:\n",
    "$$\n",
    "\\sigma_b^2(t) = w_0(t) w_1(t) [\\mu_1(t) - \\mu_0(t)]^2\n",
    "$$\n",
    "\n",
    "**K-means objective**:\n",
    "$$\n",
    "\\min_S \\sum_{i=1}^{k} \\sum_{x \\in S_i} \\|x - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "### OpenCV Functions Reference\n",
    "\n",
    "| Operation | Function | Key Parameters |\n",
    "|-----------|----------|----------------|\n",
    "| Simple threshold | `cv2.threshold(img, thresh, maxval, type)` | THRESH_BINARY, THRESH_OTSU |\n",
    "| Adaptive | `cv2.adaptiveThreshold(img, maxval, method, type, blockSize, C)` | ADAPTIVE_THRESH_MEAN_C |\n",
    "| Watershed | `cv2.watershed(img, markers)` | Needs 32-bit markers |\n",
    "| GrabCut | `cv2.grabCut(img, mask, rect, bgd, fgd, iter, mode)` | GC_INIT_WITH_RECT |\n",
    "| K-means | `cv2.kmeans(data, K, None, criteria, attempts, flags)` | KMEANS_PP_CENTERS |\n",
    "| Distance transform | `cv2.distanceTransform(img, distanceType, maskSize)` | DIST_L2 |\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Otsu/Adaptive**:\n",
    "- Simple binary segmentation\n",
    "- Document processing\n",
    "- Preprocessing for other methods\n",
    "\n",
    "**Watershed**:\n",
    "- Counting objects (cells, coins, etc.)\n",
    "- Separating touching objects\n",
    "- When you can provide markers\n",
    "\n",
    "**GrabCut**:\n",
    "- Photo editing (background removal)\n",
    "- Interactive applications\n",
    "- When user can draw rectangle\n",
    "\n",
    "**K-means**:\n",
    "- Color quantization\n",
    "- Compression\n",
    "- When regions have distinct colors\n",
    "\n",
    "**Next**: Module 10 - Feature Detection (Advanced)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}