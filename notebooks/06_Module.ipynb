{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# Module 6: Feature Detection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adiel2012/computer-graphics/blob/main/notebooks/06_Module.ipynb)\n",
    "\n",
    "**Week 11-12: Harris Corners, SIFT, ORB, Feature Matching**\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand corner and keypoint detection theory\n",
    "- Implement Harris corner detector\n",
    "- Apply SIFT, SURF, and ORB feature detectors\n",
    "- Perform feature matching and descriptors\n",
    "- Build practical applications (image stitching, object recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-warning",
   "metadata": {},
   "source": [
    "---\n",
    "## ⚠️ IMPORTANT: Run All Cells in Order\n",
    "\n",
    "**This notebook must be executed sequentially from top to bottom.**\n",
    "\n",
    "- Click **Runtime → Run all** (or **Cell → Run All**)\n",
    "- Do NOT skip cells or run them out of order\n",
    "- Each cell depends on variables from previous cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-theory-intro",
   "metadata": {},
   "source": [
    "---\n",
    "## Mathematical Foundations: Feature Detection Theory\n",
    "\n",
    "### What is a Feature?\n",
    "\n",
    "A **feature** (or **keypoint**) is a distinctive, repeatable point in an image:\n",
    "- Can be reliably detected in different images\n",
    "- Invariant to transformations (rotation, scale, illumination)\n",
    "- Has a distinctive neighborhood (descriptor)\n",
    "\n",
    "### Why Feature Detection?\n",
    "\n",
    "1. **Image matching**: Find corresponding points between images\n",
    "2. **Object recognition**: Identify objects by their features\n",
    "3. **3D reconstruction**: Recover 3D structure from 2D images\n",
    "4. **Tracking**: Follow objects across video frames\n",
    "5. **Image stitching**: Create panoramas\n",
    "6. **Camera calibration**: Determine camera parameters\n",
    "\n",
    "### Feature Types\n",
    "\n",
    "1. **Corners**: Intersection of edges (high curvature)\n",
    "2. **Blobs**: Regions with distinctive intensity patterns\n",
    "3. **Edges**: Boundaries between regions\n",
    "4. **Ridges**: Elongated structures\n",
    "\n",
    "### Good Features: Desirable Properties\n",
    "\n",
    "1. **Repeatability**: Detected consistently across different views\n",
    "2. **Distinctiveness**: Each feature has unique characteristics\n",
    "3. **Locality**: Features occupy small, well-defined regions\n",
    "4. **Quantity**: Enough features for robust matching\n",
    "5. **Accuracy**: Precise localization\n",
    "6. **Efficiency**: Fast to compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-load-images",
   "metadata": {},
   "outputs": [],
   "source": "# Load test images\nimport urllib.request\nfrom urllib.request import Request, urlopen\n\n# Image with corners\nurl1 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d7/Chessboard480.svg/330px-Chessboard480.svg.png'\nreq1 = Request(url1, headers={'User-Agent': 'Mozilla/5.0'})\nwith urlopen(req1) as response:\n    image_data1 = response.read()\nwith open('chessboard.png', 'wb') as f:\n    f.write(image_data1)\n\nimg_chess = cv2.imread('chessboard.png')\nimg_chess_gray = cv2.cvtColor(img_chess, cv2.COLOR_BGR2GRAY)\n\n# Natural image\nurl2 = 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/481px-Cat03.jpg'\nreq2 = Request(url2, headers={'User-Agent': 'Mozilla/5.0'})\nwith urlopen(req2) as response:\n    image_data2 = response.read()\nwith open('box.jpg', 'wb') as f:\n    f.write(image_data2)\n\nimg_box = cv2.imread('box.jpg')\nimg_box_gray = cv2.cvtColor(img_box, cv2.COLOR_BGR2GRAY)\n\nprint(f\"Chessboard image: {img_chess.shape}\")\nprint(f\"Box image: {img_box.shape}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\naxes[0].imshow(cv2.cvtColor(img_chess, cv2.COLOR_BGR2RGB))\naxes[0].set_title('Chessboard (Many Corners)')\naxes[0].axis('off')\n\naxes[1].imshow(cv2.cvtColor(img_box, cv2.COLOR_BGR2RGB))\naxes[1].set_title('Natural Image')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n"
  },
  {
   "cell_type": "markdown",
   "id": "cell-harris-theory",
   "metadata": {},
   "source": [
    "### 1. Harris Corner Detector\n",
    "\n",
    "The **Harris corner detector** (Harris & Stephens, 1988) finds corners by analyzing local image gradients.\n",
    "\n",
    "#### Intuition: What is a Corner?\n",
    "\n",
    "Consider shifting a small window in different directions:\n",
    "- **Flat region**: No change in any direction\n",
    "- **Edge**: Change perpendicular to edge, no change along edge\n",
    "- **Corner**: Large change in all directions!\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "**Step 1**: Compute image gradients $I_x$ and $I_y$\n",
    "\n",
    "**Step 2**: Build the **structure tensor** (second moment matrix):\n",
    "\n",
    "$$\n",
    "M = \\sum_{(x,y) \\in W} \\begin{bmatrix}\n",
    "I_x^2 & I_x I_y \\\\\n",
    "I_x I_y & I_y^2\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "A & C \\\\\n",
    "C & B\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $W$ is a local window\n",
    "- Often weighted by Gaussian $G(\\sigma)$\n",
    "\n",
    "**With Gaussian weighting**:\n",
    "$$\n",
    "A = G(\\sigma) * I_x^2, \\quad B = G(\\sigma) * I_y^2, \\quad C = G(\\sigma) * (I_x I_y)\n",
    "$$\n",
    "\n",
    "**Step 3**: Compute corner response\n",
    "\n",
    "**Harris response function**:\n",
    "$$\n",
    "R = \\det(M) - k \\cdot \\text{trace}(M)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "R = AB - C^2 - k(A + B)^2\n",
    "$$\n",
    "\n",
    "Where $k$ is a sensitivity parameter (typically $k = 0.04$ to $0.06$)\n",
    "\n",
    "#### Eigenvalue Interpretation\n",
    "\n",
    "The structure tensor $M$ has eigenvalues $\\lambda_1, \\lambda_2$:\n",
    "\n",
    "$$\n",
    "\\det(M) = \\lambda_1 \\lambda_2, \\quad \\text{trace}(M) = \\lambda_1 + \\lambda_2\n",
    "$$\n",
    "\n",
    "**Classification**:\n",
    "- $\\lambda_1 \\approx \\lambda_2 \\approx 0$: Flat region\n",
    "- $\\lambda_1 \\gg \\lambda_2$ (or vice versa): Edge\n",
    "- $\\lambda_1, \\lambda_2$ both large: **Corner**\n",
    "\n",
    "**Harris response**:\n",
    "- $R > 0$: Corner\n",
    "- $R < 0$: Edge\n",
    "- $R \\approx 0$: Flat\n",
    "\n",
    "#### Algorithm Steps\n",
    "\n",
    "1. Compute gradients $I_x$, $I_y$ (e.g., using Sobel)\n",
    "2. Compute products $I_x^2$, $I_y^2$, $I_x I_y$\n",
    "3. Apply Gaussian filter to get $A$, $B$, $C$\n",
    "4. Compute response $R = AB - C^2 - k(A+B)^2$\n",
    "5. Threshold $R$ to find corners\n",
    "6. Apply non-maximum suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-harris-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harris corner detection\n",
    "print(\"=\" * 70)\n",
    "print(\"HARRIS CORNER DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Parameters\n",
    "blockSize = 2  # Neighborhood size\n",
    "ksize = 3      # Sobel kernel size\n",
    "k = 0.04       # Harris parameter\n",
    "\n",
    "# Detect corners\n",
    "harris_response = cv2.cornerHarris(img_chess_gray, blockSize, ksize, k)\n",
    "\n",
    "# Dilate to mark corners\n",
    "harris_response = cv2.dilate(harris_response, None)\n",
    "\n",
    "print(f\"\\nHarris parameters:\")\n",
    "print(f\"  blockSize: {blockSize} (neighborhood size)\")\n",
    "print(f\"  ksize: {ksize} (Sobel aperture)\")\n",
    "print(f\"  k: {k} (sensitivity parameter)\")\n",
    "print(f\"\\nResponse range: [{harris_response.min():.2e}, {harris_response.max():.2e}]\")\n",
    "\n",
    "# Threshold for corner detection\n",
    "threshold = 0.01 * harris_response.max()\n",
    "corner_img = img_chess.copy()\n",
    "corner_img[harris_response > threshold] = [0, 0, 255]  # Red\n",
    "\n",
    "num_corners = np.sum(harris_response > threshold)\n",
    "print(f\"Corners detected: {num_corners}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(img_chess, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(harris_response, cmap='hot')\n",
    "axes[1].set_title('Harris Response\\n(Brighter = Stronger Corner)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(cv2.cvtColor(corner_img, cv2.COLOR_BGR2RGB))\n",
    "axes[2].set_title(f'Detected Corners (Red)\\n{num_corners} corners')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Harris detects corners where gradients change in multiple directions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-harris-detailed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Harris algorithm step-by-step\n",
    "print(\"=\" * 70)\n",
    "print(\"HARRIS ALGORITHM: STEP-BY-STEP BREAKDOWN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use a simpler image for visualization\n",
    "test_img = img_chess_gray[:200, :200]\n",
    "\n",
    "# Step 1: Compute gradients\n",
    "Ix = cv2.Sobel(test_img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "Iy = cv2.Sobel(test_img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "print(\"Step 1: Computed gradients Ix and Iy using Sobel\")\n",
    "\n",
    "# Step 2: Compute products\n",
    "Ix2 = Ix ** 2\n",
    "Iy2 = Iy ** 2\n",
    "Ixy = Ix * Iy\n",
    "print(\"Step 2: Computed Ix², Iy², and IxIy\")\n",
    "\n",
    "# Step 3: Apply Gaussian filter\n",
    "sigma = 2.0\n",
    "A = cv2.GaussianBlur(Ix2, (5, 5), sigma)\n",
    "B = cv2.GaussianBlur(Iy2, (5, 5), sigma)\n",
    "C = cv2.GaussianBlur(Ixy, (5, 5), sigma)\n",
    "print(f\"Step 3: Applied Gaussian smoothing (σ={sigma})\")\n",
    "\n",
    "# Step 4: Compute Harris response\n",
    "k = 0.04\n",
    "det_M = A * B - C ** 2\n",
    "trace_M = A + B\n",
    "R = det_M - k * (trace_M ** 2)\n",
    "print(f\"Step 4: Computed Harris response R = det(M) - {k}×trace(M)²\")\n",
    "print(f\"  det(M) = AB - C²\")\n",
    "print(f\"  trace(M) = A + B\")\n",
    "\n",
    "# Visualize all steps\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "\n",
    "axes[0, 0].imshow(test_img, cmap='gray')\n",
    "axes[0, 0].set_title('0. Original')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(Ix, cmap='RdBu')\n",
    "axes[0, 1].set_title('1. Gradient Ix')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].imshow(Iy, cmap='RdBu')\n",
    "axes[0, 2].set_title('1. Gradient Iy')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "axes[1, 0].imshow(Ix2, cmap='hot')\n",
    "axes[1, 0].set_title('2. Ix²')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(Iy2, cmap='hot')\n",
    "axes[1, 1].set_title('2. Iy²')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].imshow(np.abs(Ixy), cmap='hot')\n",
    "axes[1, 2].set_title('2. |IxIy|')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "axes[2, 0].imshow(A, cmap='hot')\n",
    "axes[2, 0].set_title('3. A (smoothed Ix²)')\n",
    "axes[2, 0].axis('off')\n",
    "\n",
    "axes[2, 1].imshow(B, cmap='hot')\n",
    "axes[2, 1].set_title('3. B (smoothed Iy²)')\n",
    "axes[2, 1].axis('off')\n",
    "\n",
    "axes[2, 2].imshow(R, cmap='hot')\n",
    "axes[2, 2].set_title('4. Harris Response R\\n(Corners = bright spots)')\n",
    "axes[2, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Harris response is high where both eigenvalues are large!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-shi-tomasi",
   "metadata": {},
   "source": [
    "### 2. Shi-Tomasi Corner Detector\n",
    "\n",
    "**Shi-Tomasi** (1994) improved Harris by using a simpler criterion.\n",
    "\n",
    "#### Difference from Harris\n",
    "\n",
    "**Harris response**:\n",
    "$$\n",
    "R = \\lambda_1 \\lambda_2 - k(\\lambda_1 + \\lambda_2)^2\n",
    "$$\n",
    "\n",
    "**Shi-Tomasi response**:\n",
    "$$\n",
    "R = \\min(\\lambda_1, \\lambda_2)\n",
    "$$\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "1. **No parameter $k$**: Eliminates tuning\n",
    "2. **Better feature selection**: Selects best corners directly\n",
    "3. **Quality measure**: Direct geometric meaning (minimum gradient variation)\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "- **Shi-Tomasi**: Feature tracking, optical flow\n",
    "- **Harris**: General corner detection, more geometric insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-shi-tomasi-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shi-Tomasi corner detection\n",
    "print(\"=\" * 70)\n",
    "print(\"SHI-TOMASI CORNER DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Parameters\n",
    "maxCorners = 100\n",
    "qualityLevel = 0.01\n",
    "minDistance = 10\n",
    "\n",
    "# Detect corners\n",
    "corners = cv2.goodFeaturesToTrack(img_box_gray, maxCorners, qualityLevel, minDistance)\n",
    "\n",
    "print(f\"\\nShi-Tomasi parameters:\")\n",
    "print(f\"  maxCorners: {maxCorners} (maximum number of corners)\")\n",
    "print(f\"  qualityLevel: {qualityLevel} (minimum quality)\")\n",
    "print(f\"  minDistance: {minDistance} (minimum distance between corners)\")\n",
    "print(f\"\\nCorners detected: {len(corners)}\")\n",
    "\n",
    "# Draw corners\n",
    "img_corners = img_box.copy()\n",
    "if corners is not None:\n",
    "    corners = np.int32(corners)\n",
    "    for corner in corners:\n",
    "        x, y = corner.ravel()\n",
    "        cv2.circle(img_corners, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(img_box, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(img_corners, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'Shi-Tomasi Corners (Green)\\n{len(corners)} corners')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Shi-Tomasi directly selects best corners without parameter k!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-sift-theory",
   "metadata": {},
   "source": [
    "### 3. SIFT (Scale-Invariant Feature Transform)\n",
    "\n",
    "**SIFT** (Lowe, 2004) is a landmark algorithm for robust feature detection and description.\n",
    "\n",
    "#### Key Innovations\n",
    "\n",
    "1. **Scale invariance**: Detects features at multiple scales\n",
    "2. **Rotation invariance**: Assigns canonical orientation\n",
    "3. **Distinctive descriptors**: 128-dimensional feature vectors\n",
    "4. **Robustness**: Invariant to illumination, viewpoint changes\n",
    "\n",
    "#### Algorithm Overview\n",
    "\n",
    "**Step 1: Scale-Space Extrema Detection**\n",
    "\n",
    "Build **Difference of Gaussians (DoG)** pyramid:\n",
    "$$\n",
    "D(x, y, \\sigma) = [G(x, y, k\\sigma) - G(x, y, \\sigma)] * I(x, y)\n",
    "$$\n",
    "\n",
    "Find local extrema in both **space** and **scale**.\n",
    "\n",
    "**Step 2: Keypoint Localization**\n",
    "\n",
    "- Reject low-contrast points\n",
    "- Eliminate edge responses\n",
    "- Sub-pixel localization using Taylor expansion\n",
    "\n",
    "**Step 3: Orientation Assignment**\n",
    "\n",
    "Compute gradient histogram in neighborhood:\n",
    "$$\n",
    "\\theta(x, y) = \\arctan\\left(\\frac{L_y}{L_x}\\right)\n",
    "$$\n",
    "\n",
    "Assign dominant orientation(s) to achieve rotation invariance.\n",
    "\n",
    "**Step 4: Keypoint Descriptor**\n",
    "\n",
    "- Rotate neighborhood to canonical orientation\n",
    "- Divide into 4×4 subregions\n",
    "- Compute 8-bin gradient histogram per subregion\n",
    "- Result: **128-dimensional descriptor** (4×4×8)\n",
    "- Normalize for illumination invariance\n",
    "\n",
    "#### SIFT Descriptor Properties\n",
    "\n",
    "- **Distinctive**: High discrimination power\n",
    "- **Invariant**: Scale, rotation, illumination\n",
    "- **Robust**: Partial invariance to viewpoint, affine transformations\n",
    "- **Efficient**: Fast matching using nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-sift-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT feature detection\n",
    "print(\"=\" * 70)\n",
    "print(\"SIFT FEATURE DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create SIFT detector\n",
    "# Note: SIFT is patented and not available in opencv-python\n",
    "# You need opencv-contrib-python: pip install opencv-contrib-python\n",
    "try:\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_box_gray, None)\n",
    "    \n",
    "    print(f\"\\nKeypoints detected: {len(keypoints)}\")\n",
    "    print(f\"Descriptor shape: {descriptors.shape}\")\n",
    "    print(f\"  Each keypoint has a {descriptors.shape[1]}-dimensional descriptor\")\n",
    "    \n",
    "    # Analyze keypoint properties\n",
    "    scales = [kp.size for kp in keypoints]\n",
    "    angles = [kp.angle for kp in keypoints]\n",
    "    responses = [kp.response for kp in keypoints]\n",
    "    \n",
    "    print(f\"\\nKeypoint statistics:\")\n",
    "    print(f\"  Scale range: [{min(scales):.2f}, {max(scales):.2f}]\")\n",
    "    print(f\"  Angle range: [{min(angles):.2f}°, {max(angles):.2f}°]\")\n",
    "    print(f\"  Response range: [{min(responses):.4f}, {max(responses):.4f}]\")\n",
    "    \n",
    "    # Draw keypoints with different visualization options\n",
    "    img_keypoints = cv2.drawKeypoints(img_box, keypoints, None,\n",
    "                                      flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(img_box, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(img_keypoints, cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title(f'SIFT Keypoints\\n{len(keypoints)} features\\n(Circle = scale, Arrow = orientation)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize descriptor statistics\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    axes[0].hist(scales, bins=30, edgecolor='black')\n",
    "    axes[0].set_title('Keypoint Scale Distribution')\n",
    "    axes[0].set_xlabel('Scale (size)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].hist(angles, bins=36, edgecolor='black')\n",
    "    axes[1].set_title('Keypoint Orientation Distribution')\n",
    "    axes[1].set_xlabel('Angle (degrees)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Descriptor heatmap (first 20 keypoints)\n",
    "    axes[2].imshow(descriptors[:20, :], cmap='hot', aspect='auto')\n",
    "    axes[2].set_title('SIFT Descriptors\\n(First 20 keypoints)')\n",
    "    axes[2].set_xlabel('Descriptor dimension (0-127)')\n",
    "    axes[2].set_ylabel('Keypoint index')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey Insight: SIFT detects features at multiple scales with orientation!\")\n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"\\nSIFT not available. Install opencv-contrib-python:\")\n",
    "    print(\"  pip install opencv-contrib-python\")\n",
    "    print(\"\\nNote: SIFT is patented (until 2020) and requires opencv-contrib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-orb-theory",
   "metadata": {},
   "source": [
    "### 4. ORB (Oriented FAST and Rotated BRIEF)\n",
    "\n",
    "**ORB** (Rublee et al., 2011) is a fast, free alternative to SIFT.\n",
    "\n",
    "#### Components\n",
    "\n",
    "**FAST (Features from Accelerated Segment Test)**:\n",
    "- Fast corner detector\n",
    "- Compares pixel with circle of 16 neighbors\n",
    "- Corner if N contiguous pixels are brighter/darker\n",
    "\n",
    "**BRIEF (Binary Robust Independent Elementary Features)**:\n",
    "- Binary descriptor (string of 0s and 1s)\n",
    "- Based on simple intensity comparisons\n",
    "- Fast to compute and match (Hamming distance)\n",
    "\n",
    "#### ORB Improvements\n",
    "\n",
    "1. **Orientation**: Adds rotation invariance to FAST\n",
    "   - Compute intensity centroid\n",
    "   - Orientation: $\\theta = \\arctan(m_{01}/m_{10})$\n",
    "\n",
    "2. **Steered BRIEF**: Rotates BRIEF pattern according to orientation\n",
    "\n",
    "3. **Scale pyramid**: Multi-scale detection\n",
    "\n",
    "#### ORB vs SIFT\n",
    "\n",
    "| Property | SIFT | ORB |\n",
    "|----------|------|-----|\n",
    "| Speed | Slow | **Very Fast** (10-100× faster) |\n",
    "| Descriptor | 128D float | **256-bit binary** |\n",
    "| Matching | L2 distance | **Hamming distance** (faster) |\n",
    "| Patent | Patented | **Free** |\n",
    "| Rotation invariance | Yes | Yes |\n",
    "| Scale invariance | Yes | Yes (pyramid) |\n",
    "| Robustness | **Better** | Good |\n",
    "\n",
    "**Use ORB when**: Speed is critical, real-time applications\n",
    "\n",
    "**Use SIFT when**: Maximum robustness needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-orb-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB feature detection\n",
    "print(\"=\" * 70)\n",
    "print(\"ORB FEATURE DETECTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create ORB detector\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(img_box_gray, None)\n",
    "\n",
    "print(f\"\\nKeypoints detected: {len(keypoints_orb)}\")\n",
    "print(f\"Descriptor shape: {descriptors_orb.shape}\")\n",
    "print(f\"  Each keypoint has a {descriptors_orb.shape[1]}-bit binary descriptor\")\n",
    "print(f\"  Descriptor dtype: {descriptors_orb.dtype} (binary)\")\n",
    "\n",
    "# Draw keypoints\n",
    "img_keypoints_orb = cv2.drawKeypoints(img_box, keypoints_orb, None,\n",
    "                                      color=(0, 255, 0),\n",
    "                                      flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].imshow(cv2.cvtColor(img_box, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(img_keypoints_orb, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'ORB Keypoints\\n{len(keypoints_orb)} features\\n(FAST corners + BRIEF descriptors)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize binary descriptors\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.imshow(descriptors_orb[:50, :], cmap='binary', aspect='auto')\n",
    "ax.set_title('ORB Binary Descriptors\\n(First 50 keypoints, black=0, white=1)')\n",
    "ax.set_xlabel('Bit index (0-255)')\n",
    "ax.set_ylabel('Keypoint index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: ORB uses binary descriptors for FAST matching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-matching-theory",
   "metadata": {},
   "source": [
    "### 5. Feature Matching\n",
    "\n",
    "**Feature matching** finds corresponding keypoints between images.\n",
    "\n",
    "#### Matching Strategies\n",
    "\n",
    "**1. Brute-Force Matching**\n",
    "\n",
    "Compare every descriptor in image 1 with every descriptor in image 2:\n",
    "$$\n",
    "\\text{match}(i) = \\arg\\min_j d(\\text{desc}_1[i], \\text{desc}_2[j])\n",
    "$$\n",
    "\n",
    "Distance metrics:\n",
    "- **L2 (Euclidean)**: For SIFT, SURF (float descriptors)\n",
    "- **Hamming**: For ORB, BRIEF, BRISK (binary descriptors)\n",
    "\n",
    "**2. FLANN (Fast Library for Approximate Nearest Neighbors)**\n",
    "\n",
    "- Much faster for large descriptor sets\n",
    "- Approximate but highly accurate\n",
    "- Uses k-d trees or LSH (Locality-Sensitive Hashing)\n",
    "\n",
    "#### Ratio Test (Lowe's Test)\n",
    "\n",
    "Find two nearest neighbors for each descriptor:\n",
    "$$\n",
    "\\text{ratio} = \\frac{d_1}{d_2}\n",
    "$$\n",
    "\n",
    "Keep match if $\\text{ratio} < 0.7$ (or 0.75)\n",
    "\n",
    "**Rationale**: Good matches have much closer nearest neighbor than second-nearest\n",
    "\n",
    "#### Cross-Check\n",
    "\n",
    "Match is valid only if:\n",
    "- Descriptor A matches descriptor B\n",
    "- AND descriptor B matches descriptor A\n",
    "\n",
    "Reduces false matches significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-matching-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matching demonstration\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE MATCHING: ORB + BRUTE FORCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load two images of the same scene (simulated by rotating)\n",
    "img1 = img_box.copy()\n",
    "h, w = img1.shape[:2]\n",
    "center = (w // 2, h // 2)\n",
    "M = cv2.getRotationMatrix2D(center, 30, 0.9)\n",
    "img2 = cv2.warpAffine(img1, M, (w, h))\n",
    "\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect ORB features\n",
    "orb = cv2.ORB_create(nfeatures=1000)\n",
    "kp1, des1 = orb.detectAndCompute(img1_gray, None)\n",
    "kp2, des2 = orb.detectAndCompute(img2_gray, None)\n",
    "\n",
    "print(f\"\\nKeypoints in image 1: {len(kp1)}\")\n",
    "print(f\"Keypoints in image 2: {len(kp2)}\")\n",
    "\n",
    "# Brute-force matching with Hamming distance\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "\n",
    "# Sort by distance (best matches first)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "print(f\"\\nTotal matches found: {len(matches)}\")\n",
    "print(f\"Best match distance: {matches[0].distance}\")\n",
    "print(f\"Worst match distance: {matches[-1].distance}\")\n",
    "\n",
    "# Draw top 50 matches\n",
    "img_matches = cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None,\n",
    "                               flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(cv2.cvtColor(img_matches, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'ORB Feature Matching (Top 50 of {len(matches)} matches)\\nImage 2 is rotated 30° and scaled 0.9×')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Features can be matched even after rotation and scaling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-ratio-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate ratio test (Lowe's test)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOWE'S RATIO TEST FOR ROBUST MATCHING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Brute-force matcher without cross-check (to use knnMatch)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n",
    "\n",
    "# Find 2 nearest neighbors for each descriptor\n",
    "matches_knn = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "print(f\"\\nTotal keypoint pairs: {len(matches_knn)}\")\n",
    "\n",
    "# Apply ratio test\n",
    "good_matches = []\n",
    "ratio_threshold = 0.75\n",
    "\n",
    "for match_pair in matches_knn:\n",
    "    if len(match_pair) == 2:  # Need 2 neighbors\n",
    "        m, n = match_pair\n",
    "        # Lowe's ratio test\n",
    "        if m.distance < ratio_threshold * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "print(f\"Matches after ratio test (threshold={ratio_threshold}): {len(good_matches)}\")\n",
    "print(f\"Rejection rate: {100 * (1 - len(good_matches) / len(matches_knn)):.1f}%\")\n",
    "\n",
    "# Draw good matches\n",
    "img_good_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches[:50], None,\n",
    "                                   flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(cv2.cvtColor(img_good_matches, cv2.COLOR_BGR2RGB))\n",
    "plt.title(f'Matches After Ratio Test (Top 50 of {len(good_matches)})\\nRatio threshold = {ratio_threshold}')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize ratio distribution\n",
    "ratios = []\n",
    "for match_pair in matches_knn:\n",
    "    if len(match_pair) == 2:\n",
    "        m, n = match_pair\n",
    "        if n.distance > 0:\n",
    "            ratios.append(m.distance / n.distance)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(ratios, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(ratio_threshold, color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Threshold = {ratio_threshold}')\n",
    "plt.xlabel('Ratio (d1 / d2)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Distance Ratios\\n(Good matches have low ratios)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insight: Ratio test filters out ambiguous matches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Corner Detection**: Harris, Shi-Tomasi\n",
    "2. **Feature Descriptors**: SIFT (128D float), ORB (256-bit binary)\n",
    "3. **Feature Matching**: Brute-force, FLANN, ratio test\n",
    "4. **Invariance**: Scale, rotation, illumination\n",
    "\n",
    "### Detector Comparison\n",
    "\n",
    "| Method | Type | Invariance | Speed | Descriptor | Best For |\n",
    "|--------|------|------------|-------|------------|----------|\n",
    "| **Harris** | Corner | None | Fast | None | Corner detection |\n",
    "| **Shi-Tomasi** | Corner | None | Fast | None | Tracking, optical flow |\n",
    "| **SIFT** | Blob | Scale, Rotation | Slow | 128D float | Robustness critical |\n",
    "| **SURF** | Blob | Scale, Rotation | Medium | 64/128D float | Faster than SIFT |\n",
    "| **ORB** | Corner | Scale, Rotation | **Very Fast** | 256-bit binary | Real-time, mobile |\n",
    "\n",
    "### Mathematical Formulas Reference\n",
    "\n",
    "**Harris corner response**:\n",
    "$$\n",
    "R = \\det(M) - k \\cdot \\text{trace}(M)^2 = \\lambda_1\\lambda_2 - k(\\lambda_1 + \\lambda_2)^2\n",
    "$$\n",
    "\n",
    "**Shi-Tomasi response**:\n",
    "$$\n",
    "R = \\min(\\lambda_1, \\lambda_2)\n",
    "$$\n",
    "\n",
    "**Structure tensor**:\n",
    "$$\n",
    "M = \\sum_{W} \\begin{bmatrix} I_x^2 & I_xI_y \\\\ I_xI_y & I_y^2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Lowe's ratio test**:\n",
    "$$\n",
    "\\frac{d_1}{d_2} < 0.75 \\quad \\text{(keep match)}\n",
    "$$\n",
    "\n",
    "### OpenCV Functions Reference\n",
    "\n",
    "| Operation | Function | Key Parameters |\n",
    "|-----------|----------|----------------|\n",
    "| Harris | `cv2.cornerHarris(img, blockSize, ksize, k)` | k ≈ 0.04-0.06 |\n",
    "| Shi-Tomasi | `cv2.goodFeaturesToTrack(img, maxCorners, quality, minDist)` | quality ≈ 0.01 |\n",
    "| SIFT | `cv2.SIFT_create()` | Requires opencv-contrib |\n",
    "| ORB | `cv2.ORB_create(nfeatures)` | Free, fast |\n",
    "| BF Matcher | `cv2.BFMatcher(normType, crossCheck)` | NORM_L2 or NORM_HAMMING |\n",
    "| FLANN | `cv2.FlannBasedMatcher(indexParams, searchParams)` | Fast approximate |\n",
    "\n",
    "**Next**: Module 7 - Image Segmentation"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}